{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from attention import Attention\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras import Model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/kdd17/ourpped/AAPL.csv', header=None)\n",
    "\n",
    "val_idx = int(df.shape[0]*0.8)\n",
    "test_idx = int(df.shape[0]*0.9)\n",
    "time_steps = 5 \n",
    "input_dim = 11 # features\n",
    "units = 3 # hidden state units\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[29:]\n",
    "# df = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data():\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    for i in range(time_steps, val_idx):\n",
    "        X_train.append(df.iloc[i-time_steps:i, :-2])\n",
    "        # y_train.append((df.iloc[i,-2] + 1) / 2)\n",
    "        y_train.append(df.iloc[i:i+1, 11])\n",
    "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "    for i in range(val_idx, df.shape[0]):\n",
    "        X_test.append(df.iloc[i-time_steps:i, :-2])\n",
    "        # y_train.append((df.iloc[i,-2] + 1) / 2)\n",
    "        y_test.append(df.iloc[i:i+1, 11])\n",
    "    X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define/compile the model.\n",
    "model_input = Input(shape=(time_steps, input_dim))\n",
    "x = Dense(input_dim, activation='tanh')(model_input)\n",
    "x = LSTM(units, return_sequences=True)(x)\n",
    "x = Attention(units=units)(x)\n",
    "x = Dense(1)(x)\n",
    "model = Model(model_input, x)\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "vocab_size = 20000  # 빈도수 상위 2만개의 단어만 사용\n",
    "max_len = 200  # 문장의 최대 길이\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "print('훈련용 리뷰 개수 : {}'.format(len(X_train)))\n",
    "print('테스트용 리뷰 개수 : {}'.format(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array([[[1,2,3,4,5,6,7,8,9,10,11],\n",
    "                   [1,2,3,4,5,6,7,8,9,10,11]],\n",
    "                   [[1,2,3,4,5,6,7,8,9,10,11],\n",
    "                   [1,2,3,4,5,6,7,8,9,10,11]],\n",
    "                   [[1,2,3,4,5,6,7,8,9,10,11],\n",
    "                   [1,2,3,4,5,6,7,8,9,10,11]],\n",
    "                   [[1,2,3,4,5,6,7,8,9,10,11],\n",
    "                   [1,2,3,4,5,6,7,8,9,10,11]],\n",
    "                   [[1,2,3,4,5,6,7,8,9,10,11],\n",
    "                   [1,2,3,4,5,6,7,8,9,10,11]],\n",
    "                   [[1,2,3,4,5,6,7,8,9,10,11],\n",
    "                   [1,2,3,4,5,6,7,8,9,10,11]],\n",
    "                   [[1,2,3,4,5,6,7,8,9,10,11],\n",
    "                   [1,2,3,4,5,6,7,8,9,10,11]],\n",
    "                   [[1,2,3,4,5,6,7,8,9,10,11],\n",
    "                   [1,2,3,4,5,6,7,8,9,10,11]],\n",
    "                   [[1,2,3,4,5,6,7,8,9,10,11],\n",
    "                   [1,2,3,4,5,6,7,8,9,10,11]],\n",
    "                   [[1,2,3,4,5,6,7,8,9,10,11],\n",
    "                   [1,2,3,4,5,6,7,8,9,10,11]],])\n",
    "# x2 = np.array([[[1,2,3,4,5,6,7,8,9,10,11],\n",
    "#                 [1,2,3,4,5,6,7,8,9,10,11]],\n",
    "#                 [[1,2,3,4,5,6,7,8,9,10,11],\n",
    "#                 [1,2,3,4,5,6,7,8,9,10,11]],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1.reshape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = np.array([[[1,2,3],\n",
    "              [4,5,6],\n",
    "              [7,10,9],\n",
    "              [1,11,3],\n",
    "              [16,12,3]],\n",
    "              [[61,23,3],\n",
    "              [14,22,3],\n",
    "              [41,28,3],\n",
    "              [21,21,3],\n",
    "              [13,42,3]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2.reshape(5,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3= np.array([[1,2,3],\n",
    "              [4,5,6]])\n",
    "x4= np.array([[7,8,9],\n",
    "              [10,11,12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.concatenate((x3,x4), axis=1).reshape((2,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import MultiHeadAttention, Dense\n",
    "mha = MultiHeadAttention(num_heads=12, key_dim=2)\n",
    "query = np.random.rand(3, 5, 4) # (batch_size, query_elements, query_depth)\n",
    "key = np.random.rand(3, 6, 5) # (batch_size, key_elements, key_depth)\n",
    "value = np.random.rand(3, 6, 6) # (batch_size, key_elements, value_depth)\n",
    "attention = mha(query, key, value) # (batch_size, query_elements, value_depth)\n",
    "attention.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dense(1, activation='sigmoid')(attention)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "embedding_dim = 32  # 각 단어의 임베딩 벡터의 차원\n",
    "num_heads = 2  # 어텐션 헤드의 수\n",
    "dff = 32  # 포지션 와이즈 피드 포워드 신경망의 은닉층의 크기\n",
    "inputs = tf.keras.layers.Input(shape=(max_len,))\n",
    "embedding_layer = TokenAndPositionEmbedding(max_len, vocab_size, embedding_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embedding_dim, num_heads, dff)\n",
    "x = transformer_block(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "x = tf.keras.layers.Dense(20, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "outputs = tf.keras.layers.Dense(2, activation=\"softmax\")(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1021d077b7873670e1beefb758f64d1cb6dee6622cc6b5e630350531058a0bc8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a39a2dbc1b0080d8d73a667c0a770303b80bfa16b99e3cad7bfe1efe562f6dfa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
