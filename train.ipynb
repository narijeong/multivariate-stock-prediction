{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.dummy import active_children\n",
    "import os\n",
    "from re import I\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras import activations\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Lambda, Concatenate, Reshape, Add, Dropout\n",
    "from keras.layers import MultiHeadAttention\n",
    "from keras.models import Model \n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "from myattention import Attention\n",
    "from layers import Aggregate\n",
    "from load import load_data, load_cla_data\n",
    "\n",
    "tra_date = '2007-01-03'\n",
    "val_date = '2015-01-02'\n",
    "tes_date = '2016-01-04'\n",
    "\n",
    "params = {\n",
    "    \"feature_dim\": 11,\n",
    "    \"ticker_size\": 2,\n",
    "}\n",
    "\n",
    "time_steps=3\n",
    "units=11\n",
    "data_path='./data/kdd17/preprocessed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname='AAPL.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_EOD = np.genfromtxt(\n",
    "    os.path.join(data_path, fname), dtype=float, delimiter=',',skip_header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.23321000e+05, -1.23321000e+05, -1.23321000e+05, ...,\n",
       "        -1.23321000e+05, -1.23321000e+05, -1.23321000e+05],\n",
       "       [-1.23321000e+05, -1.23321000e+05, -1.23321000e+05, ...,\n",
       "        -1.23321000e+05, -1.23321000e+05, -1.23321000e+05],\n",
       "       [-1.23321000e+05, -1.23321000e+05, -1.23321000e+05, ...,\n",
       "        -1.23321000e+05, -1.23321000e+05, -1.23321000e+05],\n",
       "       ...,\n",
       "       [ 6.50903552e-03,  1.07913239e-02, -4.79620581e-03, ...,\n",
       "        -3.34275600e-02, -1.00000000e+00,  1.16760002e+02],\n",
       "       [-2.39874919e-03,  3.25535844e-03, -2.82704525e-03, ...,\n",
       "        -3.04320752e-02, -1.00000000e+00,  1.16730003e+02],\n",
       "       [ 7.16630979e-03,  1.19150147e-02, -3.36729408e-03, ...,\n",
       "        -2.11362445e-02,  0.00000000e+00,  1.15820000e+02]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_EOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_data(data_path, fname):\n",
    "single_EOD = np.genfromtxt(\n",
    "os.path.join(data_path, fname), dtype=float, delimiter=',',skip_header=False)\n",
    "\n",
    "tra_ind = 0\n",
    "val_ind = dates_index[val_date]\n",
    "tes_ind = dates_index[tes_date]\n",
    "# generate training, validation, and testing instances\n",
    "# training\n",
    "tra_pv = np.zeros([tra_num, seq, fea_dim], dtype=float)\n",
    "tra_wd = np.zeros([tra_num, seq, 5], dtype=float)\n",
    "tra_gt = np.zeros([tra_num, 1], dtype=float)\n",
    "ins_ind = 0\n",
    "for date_ind in range(tra_ind, val_ind):\n",
    "    # filter out instances without length enough history\n",
    "    if date_ind < seq:\n",
    "        continue\n",
    "    tra_pv[ins_ind] = data_EOD[tic_ind][date_ind - seq: date_ind, : -2]\n",
    "    tra_wd[ins_ind] = data_wd[date_ind - seq: date_ind, :]\n",
    "    tra_gt[ins_ind, 0] = (data_EOD[tic_ind][date_ind][-2] + 1) / 2\n",
    "    ins_ind += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read each stock file and save it \n",
    "fnames = [fname for fname in os.listdir(data_path) if\n",
    "      os.path.isfile(os.path.join(data_path, fname))]\n",
    "\n",
    "for fname in fnames:\n",
    "    load_cla_data(data_path, fname, tra_date, val_date, tes_date, date_format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = split_data(path):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = pd.read_csv('./data/index/preprocessed/snp500_p.csv', header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50  tickers selected\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['BA.csv',\n",
       " 'TM.csv',\n",
       " 'MO.csv',\n",
       " 'NVS.csv',\n",
       " 'T.csv',\n",
       " 'DCM.csv',\n",
       " 'CHL.csv',\n",
       " 'BAC.csv',\n",
       " 'PEP.csv',\n",
       " 'VALE.csv',\n",
       " 'D.csv',\n",
       " 'MRK.csv',\n",
       " 'ORCL.csv',\n",
       " 'PG.csv',\n",
       " 'AMZN.csv',\n",
       " 'INTC.csv',\n",
       " 'MMM.csv',\n",
       " 'KO.csv',\n",
       " 'UPS.csv',\n",
       " 'MSFT.csv',\n",
       " 'TOT.csv',\n",
       " 'EXC.csv',\n",
       " 'HD.csv',\n",
       " 'SO.csv',\n",
       " 'XOM.csv',\n",
       " 'CVX.csv',\n",
       " 'RDS-B.csv',\n",
       " 'CMCSA.csv',\n",
       " 'NGG.csv',\n",
       " 'BHP.csv',\n",
       " 'WFC.csv',\n",
       " 'DIS.csv',\n",
       " 'GE.csv',\n",
       " 'PTR.csv',\n",
       " 'JPM.csv',\n",
       " 'SPY.csv',\n",
       " 'GOOGL.csv',\n",
       " 'PFE.csv',\n",
       " 'DUK.csv',\n",
       " 'VZ.csv',\n",
       " 'UNH.csv',\n",
       " 'MA.csv',\n",
       " 'SYT.csv',\n",
       " 'AAPL.csv',\n",
       " 'WMT.csv',\n",
       " 'NTT.csv',\n",
       " 'RIO.csv',\n",
       " 'BRK-B.csv',\n",
       " 'DOW.csv',\n",
       " 'JNJ.csv']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames = [fname for fname in os.listdir('./data/kdd17/ourpped/') if\n",
    "          os.path.isfile(os.path.join('./data/kdd17/ourpped/', fname))]\n",
    "print(len(fnames), ' tickers selected')\n",
    "fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('./data/index/preprocessed/snp500_p.csv')\n",
    "\n",
    "index_X_train = np.array([[[1,2,3,4,5,6,7,8,9,10,11],\n",
    "                   [2,3,4,5,6,7,8,9,10,11,12],\n",
    "                   [3,4,5,6,7,8,9,10,11,12,13]],\n",
    "                   [[1,2,3,4,5,6,7,8,9,10,11],\n",
    "                   [2,3,4,5,6,7,8,9,10,11,12],\n",
    "                   [3,4,5,6,7,8,9,10,11,12,13]],\n",
    "                   [[1,2,3,4,5,6,7,8,9,10,11],\n",
    "                   [2,3,4,5,6,7,8,9,10,11,12],\n",
    "                   [3,4,5,6,7,8,9,10,11,12,13]],\n",
    "                   [[1,2,3,4,5,6,7,8,9,10,11],\n",
    "                   [2,3,4,5,6,7,8,9,10,11,12],\n",
    "                   [3,4,5,6,7,8,9,10,11,12,13]],\n",
    "                   [[1,2,3,4,5,6,7,8,9,10,11],\n",
    "                   [2,3,4,5,6,7,8,9,10,11,12],\n",
    "                   [3,4,5,6,7,8,9,10,11,12,13]],])\n",
    "stock1 = np.array([[[1,2,3,4,5,6,7,8,9,10,11],\n",
    "                [2,3,4,5,6,7,8,9,10,11,12],\n",
    "                [3,4,5,6,7,8,9,10,11,12,13]],\n",
    "                [[1,2,3,4,5,6,7,8,9,10,11],\n",
    "                [2,3,4,5,6,7,8,9,10,11,12],\n",
    "                [3,4,5,6,7,8,9,10,11,12,13]],\n",
    "                [[1,2,3,4,5,6,7,8,9,10,11],\n",
    "                [2,3,4,5,6,7,8,9,10,11,12],\n",
    "                [3,4,5,6,7,8,9,10,11,12,13]],\n",
    "                [[1,2,3,4,5,6,7,8,9,10,11],\n",
    "                [2,3,4,5,6,7,8,9,10,11,12],\n",
    "                [3,4,5,6,7,8,9,10,11,12,13]],\n",
    "                [[1,2,3,4,5,6,7,8,9,10,11],\n",
    "                [2,3,4,5,6,7,8,9,10,11,12],\n",
    "                [3,4,5,6,7,8,9,10,11,12,13]],])\n",
    "stock2 = np.array([[[1,2,3,4,5,6,7,8,9,10,11],\n",
    "                [2,3,4,5,6,7,8,9,10,11,12],\n",
    "                [3,4,5,6,7,8,9,10,11,12,13]],\n",
    "                [[1,2,3,4,5,6,7,8,9,10,11],\n",
    "                [2,3,4,5,6,7,8,9,10,11,12],\n",
    "                [3,4,5,6,7,8,9,10,11,12,13]],\n",
    "                [[1,2,3,4,5,6,7,8,9,10,11],\n",
    "                [2,3,4,5,6,7,8,9,10,11,12],\n",
    "                [3,4,5,6,7,8,9,10,11,12,13]],\n",
    "                [[1,2,3,4,5,6,7,8,9,10,11],\n",
    "                [2,3,4,5,6,7,8,9,10,11,12],\n",
    "                [3,4,5,6,7,8,9,10,11,12,13]],\n",
    "                [[1,2,3,4,5,6,7,8,9,10,11],\n",
    "                [2,3,4,5,6,7,8,9,10,11,12],\n",
    "                [3,4,5,6,7,8,9,10,11,12,13]],])\n",
    "stock_X_train_list = [stock1, stock2]\n",
    "y_train = np.array([[1,0],\n",
    "                    [1,0],\n",
    "                    [0,0],\n",
    "                    [0,1],\n",
    "                    [1,1]])\n",
    "\n",
    "test = np.array([[[1,2,3,4,5,6,7,8,9,10,11],\n",
    "                [2,3,4,5,6,7,8,9,10,11,12],\n",
    "                [3,4,5,6,7,8,9,10,11,12,13]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-29 14:58:05.573787: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2, 11)\n",
      "(None, 2, 1)\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-29 14:58:11.777891: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"110\" frequency: 2600 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 12582912 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2022-07-29 14:58:11.778081: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"110\" frequency: 2600 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 12582912 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2022-07-29 14:58:11.778227: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"110\" frequency: 2600 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 12582912 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 5s 5s/step - loss: 0.5000\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4999\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4999\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4994\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9fc511c5b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def time_axis_attention(model_input, feature_dim, units):\n",
    "    x = Dense(feature_dim, activation='tanh')(model_input)\n",
    "    x = LSTM(units, return_sequences=True)(x)\n",
    "    x = Attention(feature_dim)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    return x\n",
    "\n",
    "index_input = Input(shape=(time_steps, params['feature_dim']), name='index_input')\n",
    "index_model = time_axis_attention(index_input, params['feature_dim'], units)\n",
    "# index_model = Model(inputs=index_input, outputs=index_model)\n",
    "\n",
    "stock_input_list = [Input(shape=(time_steps, params['feature_dim'])) for stock_input in range(params['ticker_size'])]\n",
    "\n",
    "context_aggs = [None]*(params['ticker_size'])\n",
    "for i in range(params['ticker_size']):\n",
    "    stock_input = stock_input_list[i]\n",
    "    stock_model = time_axis_attention(stock_input, params['feature_dim'], units)\n",
    "    # stock_model = Model(inputs=stock_input, outputs=stock_model)\n",
    "    context_aggs[i] = Aggregate(units)([index_model, stock_model])\n",
    "\n",
    "x = context_aggs[0]\n",
    "for i in range(1, params['ticker_size']):\n",
    "    x = Concatenate()([x, context_aggs[i]])\n",
    "x = Reshape((params['ticker_size'], -1))(x)\n",
    "print(x.shape)\n",
    "\n",
    "# Data Axis context\n",
    "mha = MultiHeadAttention(num_heads=2, key_dim=2)(x, x)\n",
    "# Nonlinear Transformation\n",
    "addition = Add()([x, mha])\n",
    "x = Dense(units)(addition)\n",
    "x = Add()([addition, x])\n",
    "x = Dropout(rate=0.2)(x)\n",
    "x = activations.tanh(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "print(x.shape)\n",
    "model = Model(inputs=[index_input, *stock_input_list], outputs=x)\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "plot_model(model, to_file='./image/model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "model.fit(x =[index_X_train, stock_X_train_list], y=y_train ,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.4407476 ],\n",
       "        [0.42292118]]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_X_test = np.array([[[1,2,3,4,5,6,7,8,9,10,11],\n",
    "                   [2,3,4,5,6,7,8,9,10,11,12],\n",
    "                   [3,4,5,6,7,8,9,10,11,12,13]]])\n",
    "\n",
    "stock1_test = np.array([[[1,2,3,4,5,6,7,8,9,10,11],\n",
    "                [2,3,4,5,6,7,8,9,10,11,12],\n",
    "                [3,4,5,6,7,8,9,10,11,12,13]]])\n",
    "stock2_test = np.array([[[1,2,3,4,5,6,7,8,9,10,11],\n",
    "                [2,3,4,5,6,7,8,9,10,11,12],\n",
    "                [3,4,5,6,7,8,9,10,11,12,13]]])\n",
    "stock_X_test_list = [stock1_test, stock2_test]\n",
    "pred = model.predict([index_X_test, stock_X_test_list])\n",
    "pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "1021d077b7873670e1beefb758f64d1cb6dee6622cc6b5e630350531058a0bc8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
